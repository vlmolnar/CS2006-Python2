{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data refining</h1>\n",
    "<p>This script contains functionality to refine data and produce a new data set. The module pandas contains many built-in functions for this task which was capitalised on.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import refine as rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Data, stored in a CSV file, was provided to help us get started. The data file is read in below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/CometLanding.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Firstly, the function <i>drop_duplicates()</i> was used such that if any two rows are identical in the data set, they are recognised and the copy is removed. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77319\n"
     ]
    }
   ],
   "source": [
    "#Length of data set before removing duplicates\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(keep = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77299\n"
     ]
    }
   ],
   "source": [
    "#Length of data set after removing duplicates\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Secondly, the user's language settings (<i>user_lang</i>) are looped over. All the values in this column were changed to lowercase, so that “en-gb” and “en-GB” is recognised as the same.<br/> If the language is null but the rest of the row is reasonable, there is no point in discarding the data, and therefore <i>user_lang</i> will be set to “en” by default. If the rest of the data is damaged, it will be picked up later on.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en', 'fr', 'de', 'th', 'es', 'it', 'en-gb', 'ca', 'ru', 'ar',\n",
       "       'pt', 'nl', 'tr', 'ja', 'no', 'pl', 'ko', 'da', 'ro', 'uk', 'fi',\n",
       "       'sv', 'hu', 'cs', 'id', 'zh-cn', 'fil', 'xx-lc', 'zh-Hans', 'el',\n",
       "       'es-MX', 'zh-CN', 'en-AU', 'eu', 'en-GB', 'pt-PT', 'gl', 'he',\n",
       "       'bg', nan, 'zh-tw', 'nb', 'fa', 'vi', 'msa', 'ur', 'hi'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unique column values before, note the presesnce of 'nan' and\n",
    "#unregulated mix of lower and upper case\n",
    "df[\"user_lang\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rf.langCheck(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en', 'fr', 'de', 'th', 'es', 'it', 'en-gb', 'ca', 'ru', 'ar',\n",
       "       'pt', 'nl', 'tr', 'ja', 'no', 'pl', 'ko', 'da', 'ro', 'uk', 'fi',\n",
       "       'sv', 'hu', 'cs', 'id', 'zh-cn', 'fil', 'xx-lc', 'zh-hans', 'el',\n",
       "       'es-mx', 'en-au', 'eu', 'pt-pt', 'gl', 'he', 'bg', 'zh-tw', 'nb',\n",
       "       'fa', 'vi', 'msa', 'ur', 'hi'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Unique column values after langCheck\n",
    "df[\"user_lang\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next, the <i>removeNAN()</i> function checks and removes rows of missing data.It uses the functions <i>isnull()</i> and <i>dropna()</i> to remove NaN values (NaN is used by pandas to represent nulls). For the cases when null represents over 50% of a columns data (such as the reply column), these values are considered valid and not removed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77299\n"
     ]
    }
   ],
   "source": [
    "#Length of data set before removing null values\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rf.removeNAN(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77083\n"
     ]
    }
   ],
   "source": [
    "#Length of data set after removing null values\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Other functions were written to check for other anomalies in the data. Usernames are checked for both length and for special characters (only letters, numerals and “_” are allowed).<br/> The text column is also checked for length, as it cannot be over 140 characters. There are no cases of such corruption in the provided data set, but it may prove useful when validating other, less clean data. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rf.fromUserCheck(df)\n",
    "df = rf.textCheck(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We did not add format checking for all of the columns as that would have been tedious work, and from our observation, when the data was damaged, it usually meant that its values were null as opposed to malformed. <br/>\n",
    "Once the data set has been adequately refined, it is moved to a new file.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.makeCSV(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
